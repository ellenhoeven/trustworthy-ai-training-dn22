{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Trustworthy AI - Explainability**\n",
    "## Case study - Predictive risk assessment tool\n",
    "\n",
    "Use this notebook to explain your model predictions, both on a global model level and on an individual prediction level. \n",
    "\n",
    "##### About the use case\n",
    "\n",
    "Loans form an integral part of banking operations. However, not all the loans are promptly returned and hence it is important for a bank to closely monitor and understand loan applications so that they know which loans to reject and which to approve. \n",
    "\n",
    "This notebook explains machine learning models that use the German credit data set (https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)). It contains details of 1000 loan applicants with 20 attributes and the classification whether an applicant is considered a \"good\" or a \"bad\" credit risk (target).\n",
    "\n",
    "##### Summary\n",
    "\n",
    "This notebook demonstrates how game theoretical approaches from the Shap package can be used to explain ML models locally, and how we can use the Permutation Importance algorithm for global explanations.\n",
    "\n",
    "It also gives insights into considerations you need to take into account when choosing an explainability approach, as well as links to further in-depth readings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Explainability in ML](#Explainability_in_ML)\n",
    "\n",
    "\n",
    "2. [Import statements](#Import_statements)\n",
    "\n",
    "\n",
    "3. [Data set and model scope](#Dataset_and_model)\n",
    "  \n",
    "  \n",
    "4. [Global post-hoc explainability: permutation importance](#)\n",
    "\n",
    "\n",
    "5. [Local post-hoc explainability: Shapley values](#)\n",
    "\n",
    "\n",
    "6. [Conclusion & outlook](#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Explainability in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Machine learning explainability can be defined as the ability to explain your model's decisions, i.e. **to understand the internal workings of your model**. \n",
    "\n",
    "This can either be achieved by applying explainability algorithms to the model, or by having a directly interpretable model in the first place (e.g., a linear model). \n",
    "\n",
    "Generally, we can divide ML explanations into the following categories:\n",
    "\n",
    "1. __Global <-> local explainability__: With global explainability, we focus on understanding driving factors of the model in general, i.e. the entire model behavior. This may be, for instance, which features are important over all samples in our data set. With local explainability, we focus on understanding individual samples. \n",
    "2. __Directly interpretable <-> post-hoc__: A directly interpretable model does not need any additional explainability methods but has intrinsic explainability per se. These are models that are considered directly interpretable due to their simple structure, such as linear models or decision trees. Post hoc explainability refers to methods that are applied after model training. \n",
    "3. __Static <-> dynamic__: With a static explanation, the model is simply presented to the user. With a dynamic explanation, the user can interact with it. \n",
    "4. __Model-specific <-> model-agnostic__: Model-specific explainations are limited to model classes (e.g. weights for linear models). Model-agnostic explanations can be used on any model (e.g. permutation importance). \n",
    "    \n",
    "There are also some other considerations which we will not cover during this workshop, but you can look them up here: https://aix360.mybluemix.net/resources#guidance and at https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html \n",
    "\n",
    "For this tutorial we will be using Shapley values to show local explanations and Permutation Importance for global explanations.\n",
    "\n",
    "There are many other explainability libraries and algorithms, such as IBM Research AIX360, Lime, and others.\n",
    "You can find overviews here http://aix360.mybluemix.net/resources#guidance or here https://github.com/EthicalML/awesome-production-machine-learning#explaining-black-box-models-and-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explainability Usage Diagram**\n",
    "\n",
    "Source: https://www.ibm.com/blogs/research/2019/08/ai-explainability-360/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomy of Explainable AI  - Source: IBM Research AIX360](img/aix360taxonomy_1.png \"AIX Taxonomy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explainability Decision Tree**\n",
    "\n",
    "Source: https://www.ibm.com/blogs/research/2019/08/ai-explainability-360/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Taxonomy of Explainable AI  - Source: IBM Research AIX360](img/aix360taxonomy_2.png \"AIX Taxonomy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, enough theory, time to get started!\n",
    "\n",
    "We will be using Shap for local post-hoc explanations and ELI5 for global post-hoc explanations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sb/t9mgj_gj15ldpdr9n1gdnyfh0000gn/T/ipykernel_20263/12947450.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import shap\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/ellenhoeven/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - shap\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  shap               conda-forge/osx-64::shap-0.40.0-py39h4d6be9b_1\n",
      "  slicer             conda-forge/noarch::slicer-0.0.7-pyhd8ed1ab_0\n",
      "\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "# if you run into issues with Shap you may need to uncomment and run the following\n",
    "# !conda install -c conda-forge shap --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataset and model\n",
    "\n",
    "At this point we assume we have already trained a model that we now wish to explain. You can find the preprocessed data in *data/processed* and the trained model in *models*. The model we have trained is a random forest classifier.\n",
    "\n",
    "The data you find here is from https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data) and has already been preprocessed for you. You can find the raw data in *data/raw*. Steps that have already been done include categorical encoding and splitting into training and validation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "X_train = pickle.load(open(os.path.join(os.getcwd(), os.pardir, \"data\", \"processed\", \"german_credit_X_train.p\"), \"rb\"))\n",
    "y_train = pickle.load(open(os.path.join(os.getcwd(), os.pardir, \"data\", \"processed\", \"german_credit_y_train.p\"), \"rb\"))\n",
    "X_val = pickle.load(open(os.path.join(os.getcwd(), os.pardir, \"data\", \"processed\", \"german_credit_X_val.p\"), \"rb\"))\n",
    "y_val = pickle.load(open(os.path.join(os.getcwd(), os.pardir, \"data\", \"processed\", \"german_credit_y_val.p\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(670, 61)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(670,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(330, 61)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(330, 61)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the shapes\n",
    "display(X_train.shape)\n",
    "display(y_train.shape)\n",
    "display(X_val.shape)\n",
    "display(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains the following features:\n",
    "\n",
    "    \"Account_status\": \"Status of existing checking account\",\n",
    "    \"Duration_months\": \"Duration in months of requested loan/credit\",\n",
    "    \"Credit_history\": \"Credit history\",\n",
    "    \"Purpose\": \"Purpose of requested loan/credit\",\n",
    "    \"Credit_amount\": \"Amount of money requested\",\n",
    "    \"Savings_account\": \"Savings account/bonds available\",\n",
    "    \"Employed_since\": \"Present employment since\",\n",
    "    \"Installment_rate_percentage\": \"Monthly rate to be repayed in percentage of disposable income\",\n",
    "    \"Gender_and_status\": \"Personal status and sex\",\n",
    "    \"Other_debtors\": \"Other debtors/guarantors\",\n",
    "    \"Present_residence_since\": \"Present residence since (months)\",\n",
    "    \"Property\": \"Properties owned\",\n",
    "    \"Age\": \"Age (years)\",\n",
    "    \"Other_installments\": \"Other installment plans\",\n",
    "    \"Housing\": \"Housing status\",\n",
    "    \"Number_existing_credits\": \"Number of existing credits at this bank\",\n",
    "    \"Job\": \"Current job\",\n",
    "    \"Number_maintained_people\": \"Number of people being liable to provide maintenance for\",\n",
    "    \"Has_telephone\": \"Telephone (yes/no)\",\n",
    "    \"Is_foreign_worker\": \"Is a foreign worker (yes/no)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Duration_months</th>\n",
       "      <th>Credit_amount</th>\n",
       "      <th>Installment_rate_percentage</th>\n",
       "      <th>Present_residence_since</th>\n",
       "      <th>Age</th>\n",
       "      <th>Number_existing_credits</th>\n",
       "      <th>Number_maintained_people</th>\n",
       "      <th>Account_status_.. &lt; 0 DM</th>\n",
       "      <th>Account_status_.. &gt;= 200 DM</th>\n",
       "      <th>Account_status_0 &lt;= .. &lt; 200 DM</th>\n",
       "      <th>...</th>\n",
       "      <th>Housing_own</th>\n",
       "      <th>Housing_rent</th>\n",
       "      <th>Job_management/ self-employed/ highly qualified employee/ officer</th>\n",
       "      <th>Job_skilled employee/official</th>\n",
       "      <th>Job_unemployed/unskilled - non-resident</th>\n",
       "      <th>Job_unskilled - resident</th>\n",
       "      <th>Has_telephone_No</th>\n",
       "      <th>Has_telephone_Yes</th>\n",
       "      <th>Is_foreign_worker_No</th>\n",
       "      <th>Is_foreign_worker_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>21</td>\n",
       "      <td>2993</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>30</td>\n",
       "      <td>3656</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>12</td>\n",
       "      <td>1255</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>8</td>\n",
       "      <td>1414</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>12</td>\n",
       "      <td>691</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Duration_months  Credit_amount  Installment_rate_percentage  \\\n",
       "841               21           2993                            3   \n",
       "956               30           3656                            4   \n",
       "544               12           1255                            4   \n",
       "173                8           1414                            4   \n",
       "759               12            691                            4   \n",
       "\n",
       "     Present_residence_since  Age  Number_existing_credits  \\\n",
       "841                        2   28                        2   \n",
       "956                        4   49                        2   \n",
       "544                        4   61                        2   \n",
       "173                        2   33                        1   \n",
       "759                        3   35                        2   \n",
       "\n",
       "     Number_maintained_people  Account_status_.. < 0 DM  \\\n",
       "841                         1                         0   \n",
       "956                         1                         0   \n",
       "544                         1                         0   \n",
       "173                         1                         0   \n",
       "759                         1                         1   \n",
       "\n",
       "     Account_status_.. >= 200 DM  Account_status_0 <= .. < 200 DM  ...  \\\n",
       "841                            0                                0  ...   \n",
       "956                            1                                0  ...   \n",
       "544                            0                                0  ...   \n",
       "173                            0                                1  ...   \n",
       "759                            0                                0  ...   \n",
       "\n",
       "     Housing_own  Housing_rent  \\\n",
       "841            1             0   \n",
       "956            1             0   \n",
       "544            1             0   \n",
       "173            1             0   \n",
       "759            1             0   \n",
       "\n",
       "     Job_management/ self-employed/ highly qualified employee/ officer  \\\n",
       "841                                                  0                   \n",
       "956                                                  0                   \n",
       "544                                                  0                   \n",
       "173                                                  0                   \n",
       "759                                                  0                   \n",
       "\n",
       "     Job_skilled employee/official  Job_unemployed/unskilled - non-resident  \\\n",
       "841                              0                                        0   \n",
       "956                              0                                        0   \n",
       "544                              0                                        0   \n",
       "173                              1                                        0   \n",
       "759                              1                                        0   \n",
       "\n",
       "     Job_unskilled - resident  Has_telephone_No  Has_telephone_Yes  \\\n",
       "841                         1                 1                  0   \n",
       "956                         1                 1                  0   \n",
       "544                         1                 1                  0   \n",
       "173                         0                 1                  0   \n",
       "759                         0                 1                  0   \n",
       "\n",
       "     Is_foreign_worker_No  Is_foreign_worker_Yes  \n",
       "841                     0                      1  \n",
       "956                     0                      1  \n",
       "544                     0                      1  \n",
       "173                     1                      0  \n",
       "759                     0                      1  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the head of the validation features\n",
    "X_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary target contains information on whether someone is considered good credit risk (1) or bad credit risk (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "841    0\n",
       "956    0\n",
       "544    0\n",
       "173    0\n",
       "759    1\n",
       "Name: Risk, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the head of the validation targets\n",
    "y_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    475\n",
       "1    195\n",
       "Name: Risk, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    225\n",
       "1    105\n",
       "Name: Risk, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the class distribution in the training and validation set\n",
    "display(y_train.value_counts())\n",
    "display(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model you find here is a random forest classifier that has already been trained. \n",
    "\n",
    "If you are interested in training your own model or creating and comparing other models, you can do so by running *src/models/train_model.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to unpickle estimator MinMaxScaler from version 1.0.1 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "Trying to unpickle estimator DecisionTreeClassifier from version 1.0.1 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "Trying to unpickle estimator RandomForestClassifier from version 1.0.1 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "Trying to unpickle estimator Pipeline from version 1.0.1 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "clf = pickle.load(open(os.path.join(os.getcwd(), os.pardir, \"models\", \"german_credit_pipeline_randomForest.p\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scaler and create pipeline with classifier and scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train, y_train)\n",
    "pipe = Pipeline([(\"scaler\", scaler), (\"model\", clf)])\n",
    "pipe = Pipeline([(\"model\", clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;model&#x27;,\n",
       "                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler()),\n",
       "                                 (&#x27;model&#x27;,\n",
       "                                  RandomForestClassifier(max_features=&#x27;auto&#x27;,\n",
       "                                                         n_jobs=-1,\n",
       "                                                         random_state=50,\n",
       "                                                         verbose=1))]))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;model&#x27;,\n",
       "                 Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler()),\n",
       "                                 (&#x27;model&#x27;,\n",
       "                                  RandomForestClassifier(max_features=&#x27;auto&#x27;,\n",
       "                                                         n_jobs=-1,\n",
       "                                                         random_state=50,\n",
       "                                                         verbose=1))]))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">model: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 RandomForestClassifier(max_features=&#x27;auto&#x27;, n_jobs=-1,\n",
       "                                        random_state=50, verbose=1))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_features=&#x27;auto&#x27;, n_jobs=-1, random_state=50,\n",
       "                       verbose=1)</pre></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('model',\n",
       "                 Pipeline(steps=[('scaler', MinMaxScaler()),\n",
       "                                 ('model',\n",
       "                                  RandomForestClassifier(max_features='auto',\n",
       "                                                         n_jobs=-1,\n",
       "                                                         random_state=50,\n",
       "                                                         verbose=1))]))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Global post-hoc explainability: permutation importance\n",
    "\n",
    "For this example, we will use permutation importance from the ELI5 package (https://github.com/TeamHG-Memex/eli5/). \n",
    "\n",
    "You could also use the sklearn.inspection module.\n",
    "\n",
    "Permutation importance depends on **shuffling features** and the approach is pretty straightforward:\n",
    "1. A baseline metric is evaluated on a dataset X (in our case this is X_val)\n",
    "2. A feature column from the validation set is permuted and the evaluation metric is evaluated again. If the metric gets a lot worse, we can conclude that the feature is important: the permutaion importance is then the difference between the baseline metric and the metric with the permuted column.\n",
    "3. Repeat for all features\n",
    "\n",
    "![Permutation Importance](img/permutationimportance.png \"Permutation importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "perm = PermutationImportance(clf, random_state=42).fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0327\n",
       "                \n",
       "                    &plusmn; 0.0253\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Duration_months\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 83.79%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0242\n",
       "                \n",
       "                    &plusmn; 0.0148\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Account_status_No<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>checking<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>account\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 84.94%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0218\n",
       "                \n",
       "                    &plusmn; 0.0241\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Account_status_..<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>&lt;<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>0<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>DM\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.06%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0176\n",
       "                \n",
       "                    &plusmn; 0.0145\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Savings_account_..<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>&lt;<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>100<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>DM\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.69%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0164\n",
       "                \n",
       "                    &plusmn; 0.0165\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Credit_amount\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.09%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0103\n",
       "                \n",
       "                    &plusmn; 0.0048\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Property_building<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>society<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>savings<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>agreement/<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>life<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>insurance\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.23%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0085\n",
       "                \n",
       "                    &plusmn; 0.0024\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Number_existing_credits\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.62%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0079\n",
       "                \n",
       "                    &plusmn; 0.0048\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Savings_account_100<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>&lt;=<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>..<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>&lt;<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>500<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>DM\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.62%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0079\n",
       "                \n",
       "                    &plusmn; 0.0030\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Credit_history_no<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>credits<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>taken/all<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>credits<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>paid<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>back<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>duly\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.02%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0073\n",
       "                \n",
       "                    &plusmn; 0.0062\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Installment_rate_percentage\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.43%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0067\n",
       "                \n",
       "                    &plusmn; 0.0089\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Employed_since_1<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>&lt;=<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>..<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>&lt;<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>4<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>years\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.86%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0061\n",
       "                \n",
       "                    &plusmn; 0.0086\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Purpose_furniture/equipment\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.86%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0061\n",
       "                \n",
       "                    &plusmn; 0.0115\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Purpose_car<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>(used)\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.86%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0061\n",
       "                \n",
       "                    &plusmn; 0.0038\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Other_debtors_none\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.29%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0055\n",
       "                \n",
       "                    &plusmn; 0.0080\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Savings_account_unknown/<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>no<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>savings<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>account\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.75%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0048\n",
       "                \n",
       "                    &plusmn; 0.0082\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Gender_and_status_male_single\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.21%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0042\n",
       "                \n",
       "                    &plusmn; 0.0161\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Credit_history_critical<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>account/other<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>credits<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>existing<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>(not<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>at<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>this<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>bank)\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.21%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0042\n",
       "                \n",
       "                    &plusmn; 0.0091\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Housing_own\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.21%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0042\n",
       "                \n",
       "                    &plusmn; 0.0091\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Property_unknown<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>/<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>no<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>property<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0 0 0.1em\" title=\"A space symbol\">&emsp;</span>\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.21%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0042\n",
       "                \n",
       "                    &plusmn; 0.0082\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Job_unskilled<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>-<span style=\"background-color: hsl(120, 80%, 70%); margin: 0 0.1em 0 0.1em\" title=\"A space symbol\">&emsp;</span>resident\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 95.21%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 41 more &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_weights(perm, feature_names = X_val.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other __global__ (model-agnostic) explainability measures include, e.g., Partial Depedence Plots or Feature Interactions. \n",
    "\n",
    "You can read more about global explainability here: https://christophm.github.io/interpretable-ml-book/global-methods.html\n",
    "\n",
    "Since we are using an sklearn random forest classifier model here, you could have also used the default random forest feature importances. The permutation importance approach shown here is model-agnostic, however, and could easily be applied to other models. Also, there are some considerations to keep in mind when using the built-in feature importances (read more here: https://explained.ai/rf-importance/ and here: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Local post-hoc explainability: Shapley\n",
    "\n",
    "For this example we will use Shapley values from the Shap package (https://github.com/slundberg/shap). \n",
    "\n",
    "Shapley values are based on concepts from cooperative game theory and focus on fairly assigning the prediction results to individual features, i.e. **how each feature value contributed to the prediction**: consider a cooperative game with the same number of players as the name of features. SHAP will disclose the individual contribution of each player (or feature) on the output of the model, for each example or observation.\n",
    "\n",
    "*Important: SHAP shows the contribution of each feature, but not the quality of the prediction itself.*\n",
    "\n",
    "From an implementation perspective, the Shap package contains different explainers depending on which model you are using. We will use the KernelExplainer, which is the most general one. There are also specific Shap Explainers for e.g. tree-based methods (TreeExplainer). \n",
    "\n",
    "In order to generate Shapley values you need:\n",
    "- An average prediction on the full population (or the subsample of it), called background data\n",
    "- Predicted instances which you wish to explain using Shapley values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SHAP - example\n",
    "\n",
    "The below example shows the general concept of how Shapley values are calculated (the values are not real values and only chosen for this example). \n",
    "\n",
    "**With Shap, we answer the question: how much has each feature contributed to the prediction compared to the average prediction?**\n",
    "\n",
    "\n",
    "In our example, we assess two coalitions, in which the marginal contribution of the number of existing credits is assessed. Here, a lower loan amount leads to an increased credit score of 0,16 and thus has a marginal contribution of -0,16 in this coalition.\n",
    "\n",
    "**The Shapley value is the average marginal contribution of a feature value across all possible coalitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Shap calculation example](img/shap_example.png \"Shap example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instantiate Shap Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023627042770385742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 58,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 330,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609c759697c54c55aa16a82abb054ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.825e-05, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=3.936e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.116e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.031e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.026e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.009e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.005e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=9.922e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.769e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.336e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.306e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.233e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.231e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.050e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.268e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 50 iterations, alpha=1.103e-05, previous alpha=7.130e-06, with an active set of 25 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.262e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.981e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.495e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.443e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.092e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.062e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.931e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.789e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 9.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.728e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.665e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.649e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.527e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.625e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.487e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.367e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.211e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.925e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.737e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.427e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=5.981e-07, previous alpha=2.754e-07, with an active set of 26 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.679e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.239e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 27 iterations, alpha=1.920e-04, previous alpha=1.876e-04, with an active set of 16 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.764e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.382e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.380e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.376e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.366e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.366e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.353e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.348e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=1.315e-03, previous alpha=1.294e-03, with an active set of 10 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=5.312e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=5.302e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.539e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=2.702e-05, previous alpha=2.536e-05, with an active set of 19 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.607e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.599e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.593e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.589e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.589e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.536e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.536e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.527e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.492e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.453e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.353e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.342e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 39 iterations, alpha=1.607e-06, previous alpha=1.326e-06, with an active set of 24 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.208e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.091e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.797e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=6.417e-04, previous alpha=6.235e-04, with an active set of 13 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.991e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.351e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.349e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.347e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.346e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.346e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.345e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.343e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.343e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.343e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.330e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.329e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.744e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.731e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.712e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.711e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.710e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.696e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.690e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.679e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.670e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.669e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=6.714e-04, previous alpha=6.586e-04, with an active set of 14 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.207e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.603e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.582e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.312e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.312e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.309e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.306e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=1.968e-03, previous alpha=1.968e-03, with an active set of 13 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.821e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.737e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.480e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 57 iterations, alpha=1.153e-03, previous alpha=1.132e-03, with an active set of 40 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.988e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.104e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.104e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.639e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.582e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.578e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.567e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.557e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.551e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.530e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=6.770e-05, previous alpha=6.365e-05, with an active set of 19 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.107e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 48 iterations, alpha=4.252e-04, previous alpha=4.252e-04, with an active set of 21 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.078e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=9.862e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=9.848e-05, previous alpha=9.275e-05, with an active set of 18 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=6.860e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.855e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=1.849e-05, previous alpha=1.846e-05, with an active set of 21 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.560e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.139e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.849e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.820e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=1.814e-04, previous alpha=1.798e-04, with an active set of 17 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.441e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.207e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=7.046e-04, previous alpha=6.822e-04, with an active set of 9 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.111e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.180e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=1.159e-05, previous alpha=1.146e-05, with an active set of 22 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.174e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 56 iterations, alpha=1.272e-05, previous alpha=1.272e-05, with an active set of 25 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.369e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=9.050e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 23 iterations, alpha=8.802e-05, previous alpha=8.768e-05, with an active set of 18 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.009e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.588e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.565e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.564e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.506e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.503e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.491e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.489e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.481e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.479e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.450e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.446e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.432e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.345e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.313e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.267e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.147e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.139e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.047e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.976e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.965e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.868e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.841e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.795e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.751e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.574e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.530e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.462e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=4.467e-04, previous alpha=2.429e-04, with an active set of 22 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.391e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.717e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.695e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.030e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.014e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=3.884e-04, previous alpha=3.847e-04, with an active set of 11 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.386e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.179e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.898e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.888e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.879e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.863e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.848e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.822e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.796e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.792e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.774e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.729e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.725e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.701e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.695e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.643e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.640e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.625e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.475e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.241e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=7.823e-04, previous alpha=7.183e-04, with an active set of 15 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.064e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=4.907e-04, previous alpha=4.892e-04, with an active set of 13 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.219e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.541e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.538e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.528e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=1.541e-03, previous alpha=1.525e-03, with an active set of 15 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.324e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=5.989e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 9.599e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=5.843e-04, previous alpha=5.838e-04, with an active set of 15 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=8.733e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=8.654e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=8.653e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=8.639e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=8.606e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 9.306e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=8.603e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 9.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=8.733e-05, previous alpha=8.570e-05, with an active set of 17 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.671e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.341e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.334e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.332e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.814e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.804e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.796e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.774e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.770e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.770e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.764e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.171e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.166e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=4.171e-04, previous alpha=4.134e-04, with an active set of 17 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.823e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.403e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.399e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=1.396e-03, previous alpha=1.370e-03, with an active set of 15 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.770e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.770e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.385e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.385e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 24 iterations, alpha=3.304e-05, previous alpha=3.118e-05, with an active set of 19 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.498e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 99 iterations, alpha=3.028e-04, previous alpha=2.795e-04, with an active set of 22 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.937e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=8.990e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=8.990e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=8.606e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=8.061e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=8.921e-04, previous alpha=7.785e-04, with an active set of 19 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.083e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.088e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.080e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.078e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.068e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.068e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.066e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.058e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.054e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.046e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.036e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.029e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.028e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.026e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.006e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.998e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.981e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.724e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.678e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.673e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.668e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 27 iterations, alpha=4.839e-04, previous alpha=4.668e-04, with an active set of 18 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.936e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.604e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=1.149e-05, previous alpha=1.089e-05, with an active set of 25 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.008e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=8.387e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=8.335e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 27 iterations, alpha=4.703e-04, previous alpha=4.526e-04, with an active set of 10 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=6.818e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.453e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.445e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.444e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.811e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.807e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.806e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.746e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.609e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.606e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.358e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.344e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.447e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.423e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.419e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.389e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.152e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.990e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.562e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=9.240e-05, previous alpha=6.084e-05, with an active set of 23 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.977e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.563e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 7.224e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.442e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.420e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.315e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.292e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 51 iterations, alpha=1.503e-05, previous alpha=1.212e-05, with an active set of 26 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.401e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.029e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.029e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.515e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.514e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.218e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.218e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.217e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.217e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.211e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.211e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.210e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.210e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.210e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.205e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.204e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.203e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.197e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=1.210e-03, previous alpha=1.187e-03, with an active set of 12 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.523e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=5.202e-04, previous alpha=5.079e-04, with an active set of 8 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.483e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.391e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=6.463e-04, previous alpha=6.433e-04, with an active set of 10 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.897e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.134e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.134e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.134e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.125e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.499e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.499e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.498e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 24 iterations, alpha=5.004e-05, previous alpha=4.249e-05, with an active set of 19 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=4.209e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.765e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.763e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.760e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.749e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.733e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=1.713e-03, previous alpha=1.672e-03, with an active set of 12 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.330e-04, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.196e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.192e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=1.734e-04, previous alpha=1.733e-04, with an active set of 11 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.159e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.079e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.072e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.070e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.070e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.044e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.015e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=1.040e-03, previous alpha=1.012e-03, with an active set of 12 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.973e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.069e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 61 iterations, alpha=3.551e-04, previous alpha=3.295e-04, with an active set of 24 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.954e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.951e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=9.669e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=9.607e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=9.585e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.840e-06, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.237e-06, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 40 iterations, alpha=8.377e-06, previous alpha=7.086e-06, with an active set of 27 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.189e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.604e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.604e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.603e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.600e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.599e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.598e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.597e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.596e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.594e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.594e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.593e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.592e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.585e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.584e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.580e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.562e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.307e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.291e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.274e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.234e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.234e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.234e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.234e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.233e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.232e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.231e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.231e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.231e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.229e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.221e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.177e-05, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.177e-05, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.176e-05, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.175e-05, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.112e-05, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.423e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.273e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=6.539e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=6.165e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.521e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.590e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=4.062e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.829e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.828e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.750e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.722e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.675e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.558e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.476e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.476e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.476e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.476e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.474e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.474e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.473e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.473e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.472e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.470e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.420e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.392e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.194e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.318e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.910e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=4.846e-04, previous alpha=4.592e-04, with an active set of 9 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.408e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.982e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.862e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=6.890e-04, previous alpha=6.664e-04, with an active set of 12 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=3.714e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.165e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.165e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.536e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.050e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.008e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.695e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.839e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.490e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.188e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.104e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=7.966e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=7.776e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=7.552e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=7.388e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=6.729e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=6.042e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.173e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.028e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.228e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.161e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.970e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.960e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.645e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.547e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.522e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.267e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.192e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.093e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=9.518e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=7.543e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.997e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.932e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.932e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.328e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.234e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.188e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.861e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.831e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.751e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.695e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.135e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.852e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.403e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.327e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.664e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.170e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.152e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.146e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.141e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.133e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.133e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 24 iterations, alpha=2.165e-03, previous alpha=2.088e-03, with an active set of 15 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.140e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.179e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.175e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=1.176e-03, previous alpha=1.167e-03, with an active set of 13 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.489e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.561e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 40 iterations, alpha=2.530e-04, previous alpha=2.520e-04, with an active set of 21 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.706e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.958e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.322e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.322e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=2.142e-04, previous alpha=1.979e-04, with an active set of 17 regressors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.034e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=9.799e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=9.709e-07, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=9.799e-07, previous alpha=9.619e-07, with an active set of 27 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.029e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=6.916e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=6.167e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=6.007e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.851e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 39 iterations, alpha=6.002e-04, previous alpha=5.531e-04, with an active set of 20 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.282e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.567e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.567e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.566e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.564e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.563e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.558e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.321e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.298e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.173e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.062e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.061e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.051e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.985e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.948e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.933e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.830e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.794e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.787e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.707e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.605e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.604e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.542e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.528e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.094e-05, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.430e-05, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=8.123e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.747e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.181e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.127e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.126e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.123e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.112e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.741e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.878e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.685e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.667e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.645e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.128e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.058e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.955e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.849e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.671e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.287e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.158e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.876e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.731e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.518e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 53 iterations, alpha=6.114e-04, previous alpha=6.109e-04, with an active set of 24 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.053e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.948e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.742e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.151e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=8.675e-04, previous alpha=8.520e-04, with an active set of 11 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.681e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=3.400e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.471e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=9.162e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=9.162e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=8.671e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 53 iterations, alpha=8.552e-05, previous alpha=8.193e-05, with an active set of 22 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=9.964e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.865e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.843e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.813e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.713e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 55 iterations, alpha=2.862e-05, previous alpha=2.704e-05, with an active set of 24 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.261e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.532e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.631e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.621e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.558e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.548e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.504e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.489e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=1.554e-03, previous alpha=1.384e-03, with an active set of 11 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.835e-06, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=3.835e-06, previous alpha=3.772e-06, with an active set of 27 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.937e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.540e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.538e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.503e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.307e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.142e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.142e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.100e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.066e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.063e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.013e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=9.972e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.478e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=9.025e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=1.046e-04, previous alpha=8.822e-05, with an active set of 21 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.297e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.648e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=3.153e-03, previous alpha=3.126e-03, with an active set of 11 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.775e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.440e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.440e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.381e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.250e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.230e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.211e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.826e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.622e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.447e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.207e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.691e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.350e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.252e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.237e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.984e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.845e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.361e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.934e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.880e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.788e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.779e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.647e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.547e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.133e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.095e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.676e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.676e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.005e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=9.862e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=9.859e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=9.575e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=9.051e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.367e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.363e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=7.511e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 47 iterations, alpha=9.862e-07, previous alpha=7.313e-07, with an active set of 26 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.957e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=4.995e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=4.986e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=4.980e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.826e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.824e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.816e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.812e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.778e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.777e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.774e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.761e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 7.814e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.748e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.732e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.730e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.717e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.708e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=4.970e-04, previous alpha=4.685e-04, with an active set of 17 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.156e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.578e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.111e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.104e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.103e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.098e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.094e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.094e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.092e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.069e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=2.113e-03, previous alpha=2.068e-03, with an active set of 11 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.015e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.143e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.118e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 43 iterations, alpha=4.143e-04, previous alpha=4.113e-04, with an active set of 18 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.173e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.399e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.394e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=3.368e-04, previous alpha=3.160e-04, with an active set of 10 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.403e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.983e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.667e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.134e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=7.571e-05, previous alpha=6.972e-05, with an active set of 17 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.091e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=8.709e-05, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.780e-05, with an active set of 14 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.078e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=4.896e-05, previous alpha=4.524e-05, with an active set of 18 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.767e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.190e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.126e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=1.349e-04, previous alpha=1.187e-04, with an active set of 16 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.853e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.098e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.088e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.086e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.084e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.077e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 40 iterations, alpha=8.017e-05, previous alpha=7.902e-05, with an active set of 21 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.105e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.786e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.368e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.245e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=6.811e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=6.464e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.921e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.817e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.777e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.283e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.134e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=4.315e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=4.249e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.631e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.567e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 9.884e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.525e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.468e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.396e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.322e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.039e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.887e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.738e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.611e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.970e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.913e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.897e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.855e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.433e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.406e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.286e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.212e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=8.932e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.115e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.108e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.096e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.087e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.087e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.082e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.067e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.066e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.056e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.056e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.055e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.051e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.048e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.040e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.034e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.011e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.004e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.999e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 9.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.994e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=4.623e-04, previous alpha=3.847e-04, with an active set of 18 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.069e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.664e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.647e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.643e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.633e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.619e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.551e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.539e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.372e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.969e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.685e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.440e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.378e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.357e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.152e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.117e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.101e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.017e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.014e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.942e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.929e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.887e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.735e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.669e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=7.652e-05, previous alpha=5.284e-05, with an active set of 23 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.500e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.318e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.315e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.217e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.215e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.173e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=1.318e-05, previous alpha=1.141e-05, with an active set of 25 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.509e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.865e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.794e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.760e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.725e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.509e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.346e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.328e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.219e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.215e-05, previous alpha=1.201e-05, with an active set of 24 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.152e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.551e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.657e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.034e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=6.849e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=6.408e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=6.353e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=5.913e-05, previous alpha=4.456e-05, with an active set of 22 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.249e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.225e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=5.195e-04, previous alpha=5.125e-04, with an active set of 12 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.489e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.729e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.365e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.032e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=1.018e-04, previous alpha=9.952e-05, with an active set of 17 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.236e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.014e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.916e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.984e-04, previous alpha=3.725e-04, with an active set of 19 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.378e-05, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.429e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.161e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.025e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.864e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.801e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.587e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.578e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.564e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.493e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.486e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.283e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.008e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=9.200e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=7.776e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=7.633e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=7.165e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=7.121e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.833e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.760e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.766e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.125e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.105e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.489e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.457e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.257e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.091e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.860e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.591e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.121e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.959e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.926e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.857e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.818e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.695e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.585e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.194e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.802e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.605e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.795e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=8.477e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=7.015e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=7.008e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.961e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.764e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.638e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.810e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.810e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.627e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.204e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.580e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.457e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.558e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.415e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.415e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.935e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.241e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=6.732e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.800e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.800e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.762e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.157e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.769e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.197e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.109e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.989e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.872e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=2.250e-05, previous alpha=1.741e-05, with an active set of 22 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.442e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.159e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.151e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.144e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=1.101e-04, previous alpha=1.096e-04, with an active set of 15 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.163e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.013e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.984e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.963e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.629e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.589e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.489e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.179e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.644e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.428e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.408e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.344e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.238e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.218e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.130e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.111e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.956e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.489e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.484e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.310e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.225e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.219e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.132e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=9.449e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.274e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.388e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.567e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.000e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.681e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.168e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.000e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.500e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.566e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=1.029e-03, previous alpha=9.566e-04, with an active set of 9 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.336e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.432e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.432e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=6.430e-04, previous alpha=6.415e-04, with an active set of 12 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.042e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.863e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.214e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.314e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.313e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.313e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.310e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.307e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.306e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.305e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.302e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.297e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.290e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.287e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.286e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.285e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.274e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.253e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 9.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=1.313e-03, previous alpha=1.224e-03, with an active set of 18 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.998e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.900e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.886e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.884e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.883e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.883e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.882e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.878e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.875e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.224e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.869e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.827e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.821e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.812e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.806e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=1.900e-05, previous alpha=1.790e-05, with an active set of 26 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=7.048e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=6.919e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.936e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.655e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.655e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.542e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.521e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.304e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.910e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.638e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.472e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.381e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.122e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.951e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.839e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.755e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.564e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.438e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.121e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.903e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.866e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.563e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 9.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.357e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.349e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.349e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.266e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.308e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.985e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.315e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=9.951e-04, previous alpha=9.820e-04, with an active set of 11 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.912e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.921e-05, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=6.359e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.014e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.007e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.503e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.481e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.476e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.085e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.855e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.822e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.550e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.341e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.274e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.064e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.679e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.666e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.647e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.580e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.541e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.237e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.079e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=1.679e-05, previous alpha=9.322e-06, with an active set of 23 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.105e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.651e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.149e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.149e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.273e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=2.563e-04, previous alpha=2.212e-04, with an active set of 14 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.148e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.148e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=2.137e-05, previous alpha=2.109e-05, with an active set of 21 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.291e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.489e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.480e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.462e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.457e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.425e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.324e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.323e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.277e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.595e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.451e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.372e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.039e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.702e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.755e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.466e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.176e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.159e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.138e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.087e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=9.164e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=7.915e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.722e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.123e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.360e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.627e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.814e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=8.403e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=8.219e-05, previous alpha=7.845e-05, with an active set of 19 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.990e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 55 iterations, alpha=8.218e-04, previous alpha=8.155e-04, with an active set of 16 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.625e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.812e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.284e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=1.844e-04, previous alpha=1.771e-04, with an active set of 11 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.356e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.178e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=9.916e-04, previous alpha=9.851e-04, with an active set of 12 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.574e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.287e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=9.640e-04, previous alpha=9.040e-04, with an active set of 8 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.614e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.070e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.039e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.515e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.443e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.284e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.255e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=6.101e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=6.018e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.989e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.965e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.715e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.482e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.454e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.956e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.900e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.668e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.599e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.541e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.503e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.449e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.432e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.184e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.049e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.366e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.198e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.129e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.126e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.710e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.302e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.152e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=9.225e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=9.223e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=6.292e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=6.274e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.895e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=7.338e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=6.875e-04, previous alpha=6.524e-04, with an active set of 17 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.482e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=5.867e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=5.071e-05, previous alpha=4.952e-05, with an active set of 21 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=8.589e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.291e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.291e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.290e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.290e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.287e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=4.291e-04, previous alpha=4.285e-04, with an active set of 13 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.937e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=5.523e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 41 iterations, alpha=3.612e-04, previous alpha=3.595e-04, with an active set of 16 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.342e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.710e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.391e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.202e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=3.195e-04, previous alpha=3.183e-04, with an active set of 14 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.294e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.407e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.223e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 44 iterations, alpha=6.407e-04, previous alpha=6.114e-04, with an active set of 21 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.086e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.433e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.429e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.427e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.425e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.423e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.422e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.413e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.413e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=5.433e-05, previous alpha=5.407e-05, with an active set of 20 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.180e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=8.252e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.697e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.682e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.669e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 49 iterations, alpha=4.953e-04, previous alpha=4.754e-04, with an active set of 16 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.165e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.083e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.083e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.083e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.081e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.074e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.064e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.063e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.039e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.033e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.021e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=1.561e-05, previous alpha=1.467e-05, with an active set of 20 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.308e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.154e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.956e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.947e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=1.955e-03, previous alpha=1.943e-03, with an active set of 12 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.599e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.234e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.225e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.018e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=5.801e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 24 iterations, alpha=5.886e-05, previous alpha=5.528e-05, with an active set of 21 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.092e-06, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.650e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.550e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.389e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.259e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.238e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.190e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.021e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.763e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.511e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.307e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.253e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.116e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.747e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.292e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.117e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.798e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.705e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.265e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.419e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.059e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.778e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.983e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.598e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.233e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.317e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=4.253e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=9.631e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=9.508e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=9.150e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.643e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.490e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.106e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.463e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.263e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=6.323e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.637e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.304e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.312e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.918e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.749e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.928e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.649e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.344e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.179e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.022e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.846e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.627e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.249e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.122e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.879e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.673e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.024e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.856e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.768e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.736e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.644e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.298e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.810e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.533e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.404e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.369e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.236e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.911e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.842e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.424e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.264e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.714e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.452e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.447e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.440e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.224e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.164e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.131e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.045e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=7.317e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=6.338e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=6.293e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.885e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.405e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.156e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.133e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.597e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.002e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.992e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.640e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.368e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.196e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.116e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.053e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.778e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.767e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.511e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.462e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.396e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.317e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=9.475e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=9.032e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=8.528e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=7.823e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=7.237e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=7.146e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.920e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.707e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.521e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.310e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.727e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=7.361e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=7.361e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=7.356e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=7.329e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=7.314e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=7.309e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=7.333e-04, previous alpha=7.203e-04, with an active set of 15 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.876e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.292e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.282e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.135e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.128e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.080e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.014e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=9.960e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=9.585e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=9.338e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.746e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.517e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.385e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.966e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.576e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.276e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.128e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.050e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.682e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.498e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.114e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.054e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.542e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.178e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.396e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.353e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.000e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.000e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.837e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.817e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.687e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.041e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.033e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.403e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.135e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.135e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.649e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.373e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.258e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.125e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=6.812e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=6.579e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.767e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.718e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=6.711e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=6.648e-04, previous alpha=6.379e-04, with an active set of 18 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=3.306e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.601e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.518e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.491e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.481e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.477e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.429e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.399e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.381e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.378e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.323e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.317e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=1.601e-05, previous alpha=1.309e-05, with an active set of 22 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.913e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.802e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=1.963e-04, previous alpha=1.914e-04, with an active set of 15 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=8.859e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.850e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.831e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.830e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.819e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.809e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.808e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.806e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.800e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.790e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.788e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.783e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.754e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.749e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.743e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.736e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.735e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.723e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.722e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.695e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.693e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 8.093e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.687e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.677e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.658e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.654e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.629e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.089e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.082e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.081e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.076e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.072e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.071e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.070e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.068e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.064e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.063e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.061e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.048e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.046e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.044e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.040e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.040e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.035e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.035e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.023e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.022e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 8.093e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.019e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.015e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.007e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.005e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.994e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.608e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.572e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.569e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.548e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.528e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.525e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.521e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.510e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.491e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.486e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.475e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.419e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.409e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.397e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.383e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.381e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.357e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.356e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.302e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.298e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.093e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.285e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.267e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.228e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.221e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.171e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.101e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.652e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.370e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.365e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.364e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.361e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.358e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.358e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.357e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.356e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.353e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.352e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.351e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.343e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.341e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.340e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.338e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.337e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.334e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.334e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.326e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.326e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.093e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.324e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.321e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.316e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.316e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.315e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.307e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.171e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.763e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.693e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.683e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.682e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.677e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.671e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.670e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.669e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.666e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.660e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.659e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.656e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.640e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.638e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.634e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.630e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.630e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.623e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.623e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.608e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.607e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.093e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.603e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.598e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.587e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.587e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.585e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.571e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.303e-07, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.221e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.684e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.342e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=1.115e-04, previous alpha=1.016e-04, with an active set of 16 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.055e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.844e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=9.228e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=9.216e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=9.191e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=9.186e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=9.185e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=9.185e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=9.130e-04, previous alpha=9.061e-04, with an active set of 10 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.204e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.817e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=4.814e-05, previous alpha=4.689e-05, with an active set of 19 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.902e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 40 iterations, alpha=1.436e-04, previous alpha=1.436e-04, with an active set of 19 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.460e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.565e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.230e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.064e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.984e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=2.134e-05, previous alpha=1.678e-05, with an active set of 22 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.565e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.077e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=2.076e-03, previous alpha=2.062e-03, with an active set of 14 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.664e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=9.447e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=9.430e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=9.411e-04, previous alpha=9.232e-04, with an active set of 17 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.012e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=9.069e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.986e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.290e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=4.290e-05, previous alpha=4.285e-05, with an active set of 19 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.884e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.862e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.855e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.827e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.824e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=9.640e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=9.379e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=9.345e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=9.331e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=9.315e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=9.180e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=9.123e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.947e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.898e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.896e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.894e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.781e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.751e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.738e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.709e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.539e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.520e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.470e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.467e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.456e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.428e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=8.038e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=7.803e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=7.576e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=7.395e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=7.342e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.559e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.333e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=3.692e-06, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 45 iterations, alpha=9.463e-06, previous alpha=2.262e-06, with an active set of 26 regressors.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.852e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.848e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.836e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.836e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.832e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.093e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.831e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.823e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.805e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.804e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.788e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.788e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.787e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.782e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.779e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.751e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.748e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.694e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.675e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.614e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.605e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.597e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.580e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.501e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.484e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.466e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.461e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.367e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.343e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.240e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.305e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.086e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.375e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.360e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.337e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.337e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.335e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.331e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.655e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.514e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.484e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.435e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.433e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.169e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.118e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=5.303e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=5.293e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=5.149e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.937e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.877e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=6.521e-05, previous alpha=4.447e-05, with an active set of 21 regressors.\n"
     ]
    }
   ],
   "source": [
    "# predict probabilities\n",
    "f = lambda x: pipe.predict_proba(x)[:,1]\n",
    "# instantiate explainer on model and training samples\n",
    "# for a sklearn pipeline we need to use the Kernel Explainer\n",
    "# this might take a while to run\n",
    "# if it's too slow, you can sample a smaller subset of the background data\n",
    "explainer = shap.KernelExplainer(f, X_train.sample(50))\n",
    "# calculate shapley values on validation saamples\n",
    "shap_values = explainer.shap_values(X_val, nsamples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the local Shap explanations for each individual sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of the sample to view\n",
    "idx_to_show = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what the actual prediction was\n",
    "# Remember that 0 is bad credit risk, 1 is good credit risk\n",
    "y_val.iloc[idx_to_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shap explainer shows the average predicted probability for our dataset, and which features have lowered or highered the risk for the individual prediction. Feel free to experiment with different indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id='iCCPETS09IR7UBG4TEF99'>\n",
       "<div style='color: #900; text-align: center;'>\n",
       "  <b>Visualization omitted, Javascript library not loaded!</b><br>\n",
       "  Have you run `initjs()` in this notebook? If this notebook was from another\n",
       "  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n",
       "  this notebook on github the Javascript has been stripped for security. If you are using\n",
       "  JupyterLab this error is because a JupyterLab extension has not yet been written.\n",
       "</div></div>\n",
       " <script>\n",
       "   if (window.SHAP) SHAP.ReactDom.render(\n",
       "    SHAP.React.createElement(SHAP.AdditiveForceVisualizer, {\"outNames\": [\"f(x)\"], \"baseValue\": 0.2688, \"outValue\": 0.02999999999999997, \"link\": \"identity\", \"featureNames\": [\"Duration_months\", \"Credit_amount\", \"Installment_rate_percentage\", \"Present_residence_since\", \"Age\", \"Number_existing_credits\", \"Number_maintained_people\", \"Account_status_.. < 0 DM\", \"Account_status_.. >= 200 DM\", \"Account_status_0 <= .. < 200 DM\", \"Account_status_No checking account\", \"Credit_history_all credits at this bank paid back duly\", \"Credit_history_critical account/other credits existing (not at this bank)\", \"Credit_history_delay in paying off in the past\", \"Credit_history_existing credits paid back duly till now\", \"Credit_history_no credits taken/all credits paid back duly\", \"Purpose_business\", \"Purpose_car (new)\", \"Purpose_car (used)\", \"Purpose_domestic appliances\", \"Purpose_education\", \"Purpose_furniture/equipment\", \"Purpose_others \", \"Purpose_radio/television\", \"Purpose_repairs\", \"Purpose_retraining\", \"Savings_account_.. < 100 DM\", \"Savings_account_.. >= 1000 DM\", \"Savings_account_100 <= .. < 500 DM\", \"Savings_account_500 <= .. <= 1000 DM\", \"Savings_account_unknown/ no savings account\", \"Employed_since_.. < 1 year\", \"Employed_since_.. >= 7 years\", \"Employed_since_1 <= .. < 4 years\", \"Employed_since_4 <= .. < 7 years\", \"Employed_since_unemployed\", \"Gender_and_status_female_divorced/married\", \"Gender_and_status_male_divorced\", \"Gender_and_status_male_married/widowed\", \"Gender_and_status_male_single\", \"Other_debtors_co-applicant\", \"Other_debtors_guarantor\", \"Other_debtors_none\", \"Property_building society savings agreement/ life insurance\", \"Property_car or other\", \"Property_real estate\", \"Property_unknown / no property \", \"Other_installments_bank\", \"Other_installments_none\", \"Other_installments_stores\", \"Housing_for free\", \"Housing_own\", \"Housing_rent\", \"Job_management/ self-employed/ highly qualified employee/ officer\", \"Job_skilled employee/official\", \"Job_unemployed/unskilled - non-resident\", \"Job_unskilled - resident\", \"Has_telephone_No\", \"Has_telephone_Yes\", \"Is_foreign_worker_No\", \"Is_foreign_worker_Yes\"], \"features\": {\"0\": {\"effect\": -0.03933629629629609, \"value\": 12.0}, \"4\": {\"effect\": -0.012037037037037027, \"value\": 61.0}, \"7\": {\"effect\": -0.03263037037037034, \"value\": 0.0}, \"10\": {\"effect\": -0.0933866666666667, \"value\": 1.0}, \"12\": {\"effect\": -0.023921481481481453, \"value\": 1.0}, \"29\": {\"effect\": -0.007445925925925953, \"value\": 0.0}, \"47\": {\"effect\": -0.031628148148148294, \"value\": 0.0}, \"53\": {\"effect\": 0.0015859259259258518, \"value\": 0.0}}, \"plot_cmap\": \"RdBu\", \"labelMargin\": 20}),\n",
       "    document.getElementById('iCCPETS09IR7UBG4TEF99')\n",
       "  );\n",
       "</script>"
      ],
      "text/plain": [
       "<shap.plots._force.AdditiveForceVisualizer at 0x186946460>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap.force_plot(base_value=explainer.expected_value, shap_values=shap_values[idx_to_show], features=X_val.iloc[idx_to_show,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other __local__ (model-agnostic) methods include, e.g., LIME, Anchors or Counterfactual Explanations.\n",
    "\n",
    "You can read more about local explainability here: https://christophm.github.io/interpretable-ml-book/local-methods.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used Shap here for local explanations. You can however also use it for global explanations. This will then aggregate all of the marginal feature contributions and helps us to understand the importance or contribution of the features for the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x950 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.summary_plot(shap_values, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, high values of the duration of the loan have a high positive contribution on the prediction, while low values have a high negative contribution.\n",
    "\n",
    "For the variable housing_own, it is the other way around: high values of the variable have a high positive contribution on the prediction, while low values have a high negative contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For questions about this notebook please reach out to ellen.hoeven@ibm.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
